{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balienig\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "#logs_path = './rnn_words'\n",
    "#writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec loading...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vec/reverse_dictionary.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-71c09fa8c33d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vec loading...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreverse_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec/reverse_dictionary.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfinal_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec/final_embeddings.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vec/reverse_dictionary.txt'"
     ]
    }
   ],
   "source": [
    "print(\"vec loading...\")\n",
    "reverse_dictionary = json.load(open(\"word2vec/reverse_dictionary.txt\"))\n",
    "dictionary = {v: int(k) for k, v in reverse_dictionary.items()}\n",
    "final_embeddings = np.loadtxt('word2vec/final_embeddings.txt')\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 3\n",
    "training_iters = 500000\n",
    "n_hidden = 512\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    '''\n",
    "     open and read text file\n",
    "    '''\n",
    "    text = open(file_name, 'r').readline()\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(word):\n",
    "    value = final_embeddings[dictionary[word]]\n",
    "    return value.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(input_words,output_word,len_unique_words):\n",
    "    \n",
    "    dic, re_dic = build_dataset(text)\n",
    "    \n",
    "    train_data = []\n",
    "    target_data = np.zeros((len(input_words), len_unique_words), dtype=float)\n",
    "    \n",
    "    for i in range(0,len(input_words)):\n",
    "        vector = []\n",
    "        for j in range(0,len(input_words[i])):\n",
    "            vec = word_vector(input_words[i][j])\n",
    "            vector.append(vec)\n",
    "        train_data.append(vector)\n",
    "\n",
    "    for i in range(0, len(output_word)):\n",
    "        index = dic[output_word[i]]\n",
    "        target_data[i][index] = 1.0\n",
    "    \n",
    "    return np.array(train_data), target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text, training_iters):\n",
    "\n",
    "    unique_words = list(set(text))\n",
    "    len_unique_words = len(unique_words)\n",
    "    \n",
    "    input_words = []\n",
    "    output_word = []\n",
    "    \n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    \n",
    "    for i in range(0,training_iters):\n",
    "        \n",
    "        if offset > (len(text)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "        \n",
    "        input_words.append(text[offset:offset+n_input])\n",
    "        output_word.append(text[offset+n_input])\n",
    "        \n",
    "        offset += 1\n",
    "        \n",
    "    return input_words, output_word, unique_words, len_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 97: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c436ad959083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\balienig\\\\Documents\\\\PJ-cPeople-master\\\\train\\\\trainRnn\\\\data\\\\quotes_.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_unique_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m700000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(len(a))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(input_words[:100])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(output_words[:100])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-65f4e0f72ab2>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m      \u001b[0mopen\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     '''\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 97: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "text = read_data(\"C:\\\\Users\\\\balienig\\\\Documents\\\\PJ-cPeople-master\\\\train\\\\trainRnn\\\\data\\\\quotes_.txt\")\n",
    "a, b, unique_words, len_unique_words = featurize(text,700000)\n",
    "#print(len(a))\n",
    "#print(input_words[:100])\n",
    "#print(output_words[:100])\n",
    "#train_data, target_data = embedding(input_words, output_words, len_unique_words)\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = []\n",
    "output_words = []\n",
    "for i in range(0,len(a)):\n",
    "    if a[i][0] != '.' and a[i][1] != '.':\n",
    "        input_words.append(a[i])\n",
    "        output_words.append(b[i])\n",
    "\n",
    "input_words = input_words[:500000]\n",
    "output_words = output_words[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, target_data = embedding(input_words, output_words, len_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(x, weight, bias):\n",
    "    '''\n",
    "     define rnn cell and prediction\n",
    "    '''\n",
    "\n",
    "    x = tf.reshape(x, [-1, n_input * 128])\n",
    "    x = tf.split(x, n_input, 1)\n",
    "    \n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "    prediction = tf.matmul(outputs[-1], weight) + bias\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic, re_dic = build_dataset(text)\n",
    "\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "display_step = 1000\n",
    "    \n",
    "x = tf.placeholder(\"float\", [None, n_input, 128])\n",
    "y = tf.placeholder(\"float\", [None, len_unique_words])\n",
    "    \n",
    "weight = tf.Variable(tf.random_normal([n_hidden, len_unique_words]))\n",
    "bias = tf.Variable(tf.random_normal([len_unique_words]))\n",
    "\n",
    "logits = rnn(x, weight, bias)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "cost = tf.reduce_mean(softmax)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allresult = []\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init_op)\n",
    "\n",
    "    #writer.add_graph(session.graph)\n",
    "    \n",
    "    for i in range(0,len(train_data)):\n",
    "        \n",
    "        input_data = np.reshape(train_data[i], [-1, n_input, 128])\n",
    "        output_data = np.reshape(target_data[i], [1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, prediction] ,feed_dict={x:input_data, y:output_data})\n",
    "        \n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        \n",
    "        \n",
    "        if (i+1) % display_step == 0 :\n",
    "            result = []\n",
    "            strr = str(i+1)\n",
    "            l = loss_total/display_step\n",
    "            print(\"Iter= \" + strr + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(l) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            \n",
    "            result.append(strr)\n",
    "            result.append(l)\n",
    "            \n",
    "            symbols_out_pred = re_dic[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (input_words[i],output_words[i],symbols_out_pred))\n",
    "            \n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            \n",
    "            allresult.append(result)\n",
    "    \n",
    "    save_path = saver.save(session, \"./save_lstm1/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datatest = []\n",
    "datas = open('/Users/orathai/rnn/Deepcut/datatest.txt')\n",
    "datas = datas.readlines()\n",
    "datas = [data[:-1].split(\" \") for data in datas]\n",
    "print(datas[:10])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/model-1.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    input_words = []\n",
    "    words = datas[0]\n",
    "    for word in words:\n",
    "        input_words.append(word_vector(word))\n",
    "    \n",
    "    input_data = np.reshape(np.array(input_words), [-1, n_input, 128]) # INPUT TO VECTOR\n",
    "    # PREDICT\n",
    "    onehot_pred = sess.run(prediction, feed_dict={x: input_data}) # 1 x 1321\n",
    "    print(onehot_pred)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
